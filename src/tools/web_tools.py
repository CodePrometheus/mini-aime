"""ç½‘ç»œæœç´¢å’Œä¿¡æ¯è·å–å·¥å…·ã€‚"""

import asyncio
import json
import logging
import os
import time
from typing import Any

import requests
from langchain_tavily import TavilySearch

from .base import BaseTool, ToolError
from .rate_limiter import ExponentialBackoff, GlobalRateLimiter


logger = logging.getLogger(__name__)


class WebSearchTool(BaseTool):
    """ç½‘ç»œæœç´¢å·¥å…·ï¼ŒåŸºäº Tavily Search å®ç°ã€‚"""

    def __init__(
        self,
        api_key: str | None = None,
        max_results: int = 5,
        search_depth: str = "advanced",
        include_answer: bool = True,
        include_raw_content: bool = False,
    ):
        super().__init__(
            name="web_search",
            description="ä½¿ç”¨ AI ä¼˜åŒ–çš„æœç´¢å¼•æ“è¿›è¡Œç½‘ç»œä¿¡æ¯æœç´¢",
            required_permissions=["internet_access"],
            max_results=max_results,
            search_depth=search_depth,
            include_answer=include_answer,
            include_raw_content=include_raw_content,
        )

        # è·å– API Key
        self.api_key = api_key or os.getenv("TAVILY_API_KEY")
        if not self.api_key:
            raise ToolError(
                "Tavily API key is required. Set TAVILY_API_KEY environment variable or pass api_key parameter."
            )

        # åˆå§‹åŒ– Tavily æœç´¢
        try:
            self.tavily = TavilySearch(
                tavily_api_key=self.api_key,
                max_results=max_results,
                search_depth=search_depth,
                include_answer=include_answer,
                include_raw_content=include_raw_content,
            )
        except Exception as e:
            raise ToolError(f"Failed to initialize Tavily Search: {e!s}")

    async def execute(self, query: str, **kwargs) -> dict[str, Any]:
        """
        æ‰§è¡Œç½‘ç»œæœç´¢ã€‚

        Args:
            query: æœç´¢æŸ¥è¯¢
            **kwargs: é¢å¤–çš„æœç´¢å‚æ•°

        Returns:
            æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - query: æœç´¢æŸ¥è¯¢
            - answer: AI ç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦
            - results: è¯¦ç»†æœç´¢ç»“æœåˆ—è¡¨
            - response_time: å“åº”æ—¶é—´

        Raises:
            ToolError: æœç´¢å¤±è´¥
        """
        if not query or not query.strip():
            raise ToolError("Search query cannot be empty")

        try:
            # æ‰§è¡Œæœç´¢
            result = self.tavily.run(query)

            # ç¡®ä¿ç»“æœæ˜¯å­—å…¸æ ¼å¼
            if isinstance(result, str):
                try:
                    result = json.loads(result)
                except json.JSONDecodeError:
                    # å¦‚æœä¸æ˜¯ JSONï¼ŒåŒ…è£…æˆå­—å…¸
                    result = {"query": query, "answer": result, "results": [], "response_time": 0}

            # éªŒè¯å’Œæ ‡å‡†åŒ–ç»“æœæ ¼å¼
            if not isinstance(result, dict):
                raise ToolError(f"Unexpected result format: {type(result)}")

            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            standardized_result = {
                "query": result.get("query", query),
                "answer": result.get("answer", ""),
                "results": result.get("results", []),
                "response_time": result.get("response_time", 0),
                "images": result.get("images", []),
                "follow_up_questions": result.get("follow_up_questions"),
                "request_id": result.get("request_id"),
            }

            return standardized_result

        except Exception as e:
            raise ToolError(f"Web search failed for query '{query}': {e!s}")

    def execute_sync(self, query: str, **kwargs) -> dict[str, Any]:
        """åŒæ­¥ç‰ˆæœ¬çš„ç½‘ç»œæœç´¢ã€‚"""
        import asyncio

        return asyncio.run(self.execute(query, **kwargs))

    def format_results(self, results: dict[str, Any], max_length: int = 1000) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºå¯è¯»å­—ç¬¦ä¸²ã€‚

        Args:
            results: æœç´¢ç»“æœå­—å…¸
            max_length: æœ€å¤§é•¿åº¦é™åˆ¶

        Returns:
            æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
        """
        formatted = []

        # æ·»åŠ æŸ¥è¯¢å’Œç­”æ¡ˆ
        formatted.append(f"ğŸ” æŸ¥è¯¢: {results.get('query', '')}")

        if results.get("answer"):
            answer = results["answer"]
            if len(answer) > max_length // 2:
                answer = answer[: max_length // 2] + "..."
            formatted.append(f"ğŸ’¡ ç­”æ¡ˆæ‘˜è¦: {answer}")

        # æ·»åŠ æœç´¢ç»“æœ
        search_results = results.get("results", [])
        if search_results:
            formatted.append(f"\nğŸ“‹ æœç´¢ç»“æœ ({len(search_results)} æ¡):")

            for i, result in enumerate(search_results[:3], 1):  # åªæ˜¾ç¤ºå‰3æ¡
                title = result.get("title", "æ— æ ‡é¢˜")
                url = result.get("url", "")
                content = result.get("content", "")

                # æˆªæ–­å†…å®¹
                if content and len(content) > 150:
                    content = content[:150] + "..."

                formatted.append(f"\n{i}. {title}")
                if url:
                    formatted.append(f"   ğŸ”— {url}")
                if content:
                    formatted.append(f"   ğŸ“„ {content}")

        # æ·»åŠ å“åº”æ—¶é—´
        if results.get("response_time"):
            formatted.append(f"\nâ±ï¸ å“åº”æ—¶é—´: {results['response_time']:.2f}ç§’")

        result_text = "\n".join(formatted)

        # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        if len(result_text) > max_length:
            result_text = result_text[:max_length] + "...\n[ç»“æœå·²æˆªæ–­]"

        return result_text


class BraveSearchTool(BaseTool):
    """åŸºäº Brave Search API çš„ç½‘ç»œæœç´¢å·¥å…·ï¼Œæ”¯æŒæ™ºèƒ½é€Ÿç‡é™åˆ¶å’Œé‡è¯•ã€‚"""

    # ç±»çº§åˆ«é…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
    _rate_limit = None  # æ¯ç§’è¯·æ±‚æ•°
    _rate_limiter_name = "brave_search_api"
    _max_retries = None  # æœ€å¤§é‡è¯•æ¬¡æ•°
    _retry_base_delay = None  # é‡è¯•åŸºç¡€å»¶è¿Ÿ

    def __init__(
        self,
        api_key: str | None = None,
        max_results: int = 3,
        country: str = "US",
        search_lang: str = "en",
        ui_lang: str = "en-US",
        safe_search: str = "moderate",
        freshness: str | None = None,
        text_decorations: bool = True,
        spellcheck: bool = True,
        rate_limit: float | None = None,
        max_retries: int | None = None,
    ):
        super().__init__(
            name="brave_search",
            description="ä½¿ç”¨ Brave Search API è¿›è¡Œç½‘ç»œä¿¡æ¯æœç´¢",
            required_permissions=["internet_access"],
            max_results=max_results,
            country=country,
            search_lang=search_lang,
            ui_lang=ui_lang,
            safe_search=safe_search,
            freshness=freshness,
            text_decorations=text_decorations,
            spellcheck=spellcheck,
        )

        # è·å– API Key
        self.api_key = api_key or os.getenv("BRAVE_SEARCH_API_KEY")
        if not self.api_key:
            raise ToolError(
                "Brave Search API key is required. Set BRAVE_SEARCH_API_KEY environment variable or pass api_key parameter."
            )

        # API é…ç½®
        self.base_url = "https://api.search.brave.com/res/v1/web/search"
        self.headers = {
            "Accept": "application/json",
            "Accept-Encoding": "gzip",
            "X-Subscription-Token": self.api_key,
        }

        # æœç´¢å‚æ•°
        self.max_results = max_results
        self.country = country
        self.search_lang = search_lang
        self.ui_lang = ui_lang
        self.safe_search = safe_search
        self.freshness = freshness
        self.text_decorations = text_decorations
        self.spellcheck = spellcheck

        # é€Ÿç‡é™åˆ¶é…ç½®ï¼ˆä¼˜å…ˆçº§ï¼šå‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼ï¼‰
        if rate_limit is not None:
            self.rate_limit = rate_limit
        elif BraveSearchTool._rate_limit is not None:
            self.rate_limit = BraveSearchTool._rate_limit
        else:
            # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤0.95 QPSï¼ˆç•¥ä½äº1.0ä»¥ç•™å‡ºå®‰å…¨è¾¹é™…ï¼‰
            self.rate_limit = float(os.getenv("BRAVE_SEARCH_RATE_LIMIT", "0.95"))

        # é‡è¯•é…ç½®
        if max_retries is not None:
            self.max_retries = max_retries
        elif BraveSearchTool._max_retries is not None:
            self.max_retries = BraveSearchTool._max_retries
        else:
            self.max_retries = int(os.getenv("BRAVE_SEARCH_MAX_RETRIES", "3"))

        if BraveSearchTool._retry_base_delay is not None:
            self.retry_base_delay = BraveSearchTool._retry_base_delay
        else:
            self.retry_base_delay = float(os.getenv("BRAVE_SEARCH_RETRY_BASE_DELAY", "2.0"))

        logger.debug(
            f"BraveSearchTool initialized: rate_limit={self.rate_limit} QPS, "
            f"max_retries={self.max_retries}, retry_base_delay={self.retry_base_delay}s"
        )

    async def execute(self, query: str, **kwargs) -> dict[str, Any]:
        """
        æ‰§è¡Œç½‘ç»œæœç´¢ï¼ˆå¸¦é€Ÿç‡é™åˆ¶å’Œæ™ºèƒ½é‡è¯•ï¼‰ã€‚

        Args:
            query: æœç´¢æŸ¥è¯¢
            **kwargs: é¢å¤–çš„æœç´¢å‚æ•°ï¼Œå¯ä»¥è¦†ç›–é»˜è®¤é…ç½®

        Returns:
            æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - query: æœç´¢æŸ¥è¯¢
            - web: ç½‘é¡µæœç´¢ç»“æœ
            - infobox: ä¿¡æ¯æ¡†ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            - news: æ–°é—»ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            - videos: è§†é¢‘ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            - locations: ä½ç½®ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            - response_time: å“åº”æ—¶é—´
            - retry_count: é‡è¯•æ¬¡æ•°ï¼ˆå¦‚æœæœ‰ï¼‰

        Raises:
            ToolError: æœç´¢å¤±è´¥
        """
        if not query or not query.strip():
            raise ToolError("Search query cannot be empty")

        # æ„å»ºæœç´¢å‚æ•°
        params = {
            "q": query.strip(),
            "count": kwargs.get("count", self.max_results),
            "offset": kwargs.get("offset", 0),
            "country": kwargs.get("country", self.country),
            "search_lang": kwargs.get("search_lang", self.search_lang),
            "ui_lang": kwargs.get("ui_lang", self.ui_lang),
            "safesearch": kwargs.get("safe_search", self.safe_search),
            "text_decorations": kwargs.get("text_decorations", self.text_decorations),
            "spellcheck": kwargs.get("spellcheck", self.spellcheck),
        }

        # æ·»åŠ å¯é€‰å‚æ•°
        if self.freshness or kwargs.get("freshness"):
            params["freshness"] = kwargs.get("freshness", self.freshness)

        # åˆå§‹åŒ–é‡è¯•æœºåˆ¶
        backoff = ExponentialBackoff(
            base=self.retry_base_delay, max_delay=60.0, multiplier=2.0, jitter=True
        )

        last_error = None
        retry_count = 0

        # é‡è¯•å¾ªç¯
        for attempt in range(self.max_retries + 1):
            try:
                # è·å–å…¨å±€é€Ÿç‡é™åˆ¶å™¨
                limiter = await GlobalRateLimiter.get_limiter(
                    name=self._rate_limiter_name,
                    rate=self.rate_limit,
                    capacity=max(1, int(self.rate_limit * 2)),  # å…è®¸çŸ­æ—¶çªå‘
                )

                # ç­‰å¾…é€Ÿç‡é™åˆ¶
                await limiter.acquire(tokens=1, timeout=30.0)
                logger.debug(f"Brave Search rate limit acquired for query: {query[:50]}...")

                start_time = time.time()

                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è¯·æ±‚ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: requests.get(
                        self.base_url, headers=self.headers, params=params, timeout=30
                    ),
                )

                response_time = time.time() - start_time

                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 429:
                    # é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œè§¦å‘é‡è¯•
                    retry_count = attempt
                    logger.warning(
                        f"Brave Search rate limit hit (429) on attempt {attempt + 1}/{self.max_retries + 1}, "
                        f"query: {query[:50]}..."
                    )

                    if attempt < self.max_retries:
                        delay = backoff.get_delay()
                        logger.info(f"Retrying after {delay:.2f}s...")
                        await asyncio.sleep(delay)
                        continue
                    else:
                        raise ToolError(
                            f"Rate limit exceeded after {self.max_retries} retries. "
                            "Please reduce request frequency or upgrade your Brave Search plan."
                        )

                elif response.status_code == 401:
                    raise ToolError("Invalid API key or unauthorized access.")

                elif response.status_code != 200:
                    raise ToolError(
                        f"Brave Search API error: {response.status_code} - {response.text}"
                    )

                # è§£æå“åº”
                result = response.json()

                # æ ‡å‡†åŒ–ç»“æœæ ¼å¼
                standardized_result = {
                    "query": query,
                    "web": result.get("web", {}),
                    "infobox": result.get("infobox"),
                    "news": result.get("news"),
                    "videos": result.get("videos"),
                    "locations": result.get("locations"),
                    "response_time": response_time,
                    "total_results": result.get("web", {}).get("total", 0),
                    "retry_count": retry_count,
                }

                if retry_count > 0:
                    logger.info(
                        f"Brave Search succeeded after {retry_count} retries, query: {query[:50]}..."
                    )

                return standardized_result

            except ToolError:
                # ToolError ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                raise

            except requests.RequestException as e:
                last_error = e
                retry_count = attempt
                logger.warning(
                    f"Network error on attempt {attempt + 1}/{self.max_retries + 1}: {e!s}"
                )

                if attempt < self.max_retries:
                    delay = backoff.get_delay()
                    logger.info(f"Retrying after {delay:.2f}s...")
                    await asyncio.sleep(delay)
                    continue
                else:
                    raise ToolError(
                        f"Network error during Brave search after {self.max_retries} retries "
                        f"for query '{query}': {e!s}"
                    )

            except json.JSONDecodeError as e:
                raise ToolError(f"Failed to parse Brave search response: {e!s}")

            except Exception as e:
                last_error = e
                logger.error(f"Unexpected error on attempt {attempt + 1}: {e!s}")

                if attempt < self.max_retries:
                    delay = backoff.get_delay()
                    await asyncio.sleep(delay)
                    continue
                else:
                    raise ToolError(f"Brave search failed for query '{query}': {e!s}")

        # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†ç±»å‹å®‰å…¨
        raise ToolError(f"Brave search failed after all retries: {last_error!s}")

    def execute_sync(self, query: str, **kwargs) -> dict[str, Any]:
        """åŒæ­¥ç‰ˆæœ¬çš„ç½‘ç»œæœç´¢ã€‚"""
        import asyncio

        try:
            # å°è¯•è·å–å½“å‰äº‹ä»¶å¾ªç¯
            asyncio.get_running_loop()
            # å¦‚æœå·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œåˆ›å»ºæ–°ä»»åŠ¡
            import concurrent.futures

            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, self.execute(query, **kwargs))
                return future.result()
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ asyncio.run
            return asyncio.run(self.execute(query, **kwargs))

    def format_results(self, results: dict[str, Any], max_length: int = 1000) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºå¯è¯»å­—ç¬¦ä¸²ã€‚

        Args:
            results: æœç´¢ç»“æœå­—å…¸
            max_length: æœ€å¤§é•¿åº¦é™åˆ¶

        Returns:
            æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
        """
        formatted = []

        # æ·»åŠ æŸ¥è¯¢ä¿¡æ¯
        formatted.append(f"ğŸ” æŸ¥è¯¢: {results.get('query', '')}")

        # æ·»åŠ æ€»ç»“æœæ•°
        total_results = results.get("total_results", 0)
        if total_results > 0:
            formatted.append(f"ğŸ“Š æ‰¾åˆ°çº¦ {total_results:,} æ¡ç»“æœ")

        # æ·»åŠ ä¿¡æ¯æ¡†ï¼ˆå¦‚æœæœ‰ï¼‰
        infobox = results.get("infobox")
        if infobox:
            formatted.append("\nğŸ“‹ ä¿¡æ¯æ¡†:")
            if infobox.get("title"):
                formatted.append(f"   æ ‡é¢˜: {infobox['title']}")
            if infobox.get("description"):
                desc = infobox["description"]
                if len(desc) > 200:
                    desc = desc[:200] + "..."
                formatted.append(f"   æè¿°: {desc}")

        # æ·»åŠ ç½‘é¡µæœç´¢ç»“æœ
        web_results = results.get("web", {}).get("results", [])
        if web_results:
            formatted.append("\nğŸŒ ç½‘é¡µç»“æœ:")

            for i, result in enumerate(web_results[:3], 1):  # æ˜¾ç¤ºå‰3æ¡
                title = result.get("title", "æ— æ ‡é¢˜")
                url = result.get("url", "")
                description = result.get("description", "")

                # æˆªæ–­æè¿°
                if description and len(description) > 150:
                    description = description[:150] + "..."

                formatted.append(f"\n{i}. {title}")
                if url:
                    formatted.append(f"   ğŸ”— {url}")
                if description:
                    formatted.append(f"   ğŸ“„ {description}")

        # æ·»åŠ æ–°é—»ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        news_data = results.get("news")
        if news_data and isinstance(news_data, dict):
            news_results = news_data.get("results", [])
            if news_results:
                formatted.append("\nğŸ“° æ–°é—»ç»“æœ:")
                for i, news in enumerate(news_results[:3], 1):  # æ˜¾ç¤ºå‰3æ¡æ–°é—»
                    title = news.get("title", "æ— æ ‡é¢˜")
                    url = news.get("url", "")
                    age = news.get("age", "")

                    formatted.append(f"\n{i}. {title}")
                    if url:
                        formatted.append(f"   ğŸ”— {url}")
                    if age:
                        formatted.append(f"   ğŸ“… {age}")

        # æ·»åŠ å“åº”æ—¶é—´å’Œé‡è¯•ä¿¡æ¯
        response_time = results.get("response_time")
        retry_count = results.get("retry_count", 0)

        if response_time or retry_count > 0:
            info_parts = []
            if response_time:
                info_parts.append(f"å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            if retry_count > 0:
                info_parts.append(f"é‡è¯•æ¬¡æ•°: {retry_count}")
            formatted.append(f"\nâ±ï¸ {', '.join(info_parts)}")

        result_text = "\n".join(formatted)

        # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        if len(result_text) > max_length:
            result_text = result_text[:max_length] + "...\n[ç»“æœå·²æˆªæ–­]"

        return result_text


class WebContentExtractorTool(BaseTool):
    """ç½‘é¡µå†…å®¹æå–å·¥å…·ã€‚"""

    def __init__(self):
        super().__init__(
            name="extract_web_content",
            description="ä»æŒ‡å®šURLæå–ç½‘é¡µå†…å®¹",
            required_permissions=["internet_access"],
        )

    async def execute(self, url: str, max_length: int = 5000) -> str:
        """
        æå–ç½‘é¡µå†…å®¹ã€‚

        Args:
            url: ç½‘é¡µURL
            max_length: æœ€å¤§å†…å®¹é•¿åº¦

        Returns:
            æå–çš„ç½‘é¡µå†…å®¹

        Raises:
            ToolError: å†…å®¹æå–å¤±è´¥
        """
        if not url or not url.strip():
            raise ToolError("URL cannot be empty")

        try:
            import requests
            from bs4 import BeautifulSoup

            # å‘é€HTTPè¯·æ±‚
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # è§£æHTML
            soup = BeautifulSoup(response.content, "html.parser")

            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(["script", "style"]):
                script.decompose()

            # æå–æ–‡æœ¬å†…å®¹
            text = soup.get_text()

            # æ¸…ç†æ–‡æœ¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = " ".join(chunk for chunk in chunks if chunk)

            # é™åˆ¶é•¿åº¦
            if len(text) > max_length:
                text = text[:max_length] + "...[å†…å®¹å·²æˆªæ–­]"

            return text

        except requests.RequestException as e:
            raise ToolError(f"Failed to fetch URL {url}: {e!s}")
        except Exception as e:
            raise ToolError(f"Failed to extract content from {url}: {e!s}")

    def execute_sync(self, url: str, max_length: int = 5000) -> str:
        """åŒæ­¥ç‰ˆæœ¬çš„ç½‘é¡µå†…å®¹æå–ã€‚"""
        import asyncio

        return asyncio.run(self.execute(url, max_length))


# å¯¼å‡ºçš„ç±»
__all__ = ["BraveSearchTool", "WebContentExtractorTool", "WebSearchTool"]
