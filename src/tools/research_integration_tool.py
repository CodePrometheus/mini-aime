"""ç ”ç©¶æ•´åˆå·¥å…· - æ™ºèƒ½å‘ç°å’Œæ•´åˆå­ä»»åŠ¡çš„ç ”ç©¶æˆæœã€‚"""

import json
import logging
import os
import re
from typing import Any

from .base import BaseTool, ToolError

logger = logging.getLogger(__name__)


class ResearchIntegrationTool(BaseTool):
    """ç ”ç©¶æ•´åˆå·¥å…·ï¼Œç”¨äºæ™ºèƒ½å‘ç°å’Œæ•´åˆå­ä»»åŠ¡çš„ç ”ç©¶æˆæœã€‚"""

    def __init__(self, allowed_paths: list[str] | None = None):
        super().__init__(
            name="integrate_research",
            description="æ™ºèƒ½å‘ç°å’Œæ•´åˆä»»åŠ¡ç›®å½•ä¸‹çš„æ‰€æœ‰ç ”ç©¶æ–‡ä»¶ï¼Œç”Ÿæˆç»¼åˆæŠ¥å‘Š",
            required_permissions=["file_system_access"],
            allowed_paths=allowed_paths,
        )
        self.allowed_paths = allowed_paths
        self._docs_base_dir = self._find_docs_dir()

    def _find_docs_dir(self) -> str:
        path = os.path.dirname(os.path.abspath(__file__))
        while path and path != os.path.dirname(path):
            docs_path = os.path.join(path, "docs")
            if os.path.exists(docs_path):
                return docs_path
            if os.path.exists(os.path.join(path, "pyproject.toml")):
                docs_path = os.path.join(path, "docs")
                os.makedirs(docs_path, exist_ok=True)
                return docs_path
            path = os.path.dirname(path)
        return os.path.join(os.getcwd(), "docs")

    def _resolve_task_directory(self, path: str) -> str:
        if os.path.isabs(path):
            return path
        return os.path.join(self._docs_base_dir, path)

    async def execute(
        self, task_directory: str, output_file: str, original_goal: str = "", llm_client=None
    ) -> str:
        """
        æ•´åˆç ”ç©¶æ–‡ä»¶å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Šã€‚

        Args:
            task_directory: ä»»åŠ¡ç›®å½•è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            original_goal: åŸå§‹ç›®æ ‡æè¿°
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºå†…å®¹åˆ†æï¼‰

        Returns:
            æ•´åˆç»“æœçš„æè¿°
        """
        try:
            # 1. å‘ç°ç ”ç©¶æ–‡ä»¶
            research_files = await self._discover_research_files(task_directory)

            if not research_files:
                raise ToolError(f"åœ¨ç›®å½• {task_directory} ä¸­æœªæ‰¾åˆ°ä»»ä½•ç ”ç©¶æ–‡ä»¶")

            # 2. è¯»å–å¹¶åˆ†æç ”ç©¶å†…å®¹
            research_content = await self._read_and_analyze_research(research_files, llm_client)

            # 3. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            final_report = await self._generate_comprehensive_report(
                research_content, original_goal, llm_client
            )

            # 4. ä¿å­˜æŠ¥å‘Š
            await self._save_report(final_report, output_file)

            return f"æˆåŠŸæ•´åˆ {len(research_files)} ä¸ªç ”ç©¶æ–‡ä»¶ï¼Œç”Ÿæˆç»¼åˆæŠ¥å‘Š: {output_file}"

        except Exception as e:
            raise ToolError(f"ç ”ç©¶æ•´åˆå¤±è´¥: {e!s}")

    async def _discover_research_files(self, task_directory: str) -> list[str]:
        """å‘ç°ä»»åŠ¡ç›®å½•ä¸‹çš„æ‰€æœ‰ç ”ç©¶æ–‡ä»¶ã€‚"""
        try:
            resolved_dir = self._resolve_task_directory(task_directory)
            logger.info(f"ResearchIntegrationTool: è§£æåçš„ç›®å½•è·¯å¾„: {resolved_dir}")
            
            if not os.path.exists(resolved_dir):
                raise ToolError(f"ç›®å½•ä¸å­˜åœ¨: {task_directory} (è§£æå: {resolved_dir})")

            if not os.path.isdir(resolved_dir):
                raise ToolError(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {task_directory}")

            # è·å–æ‰€æœ‰æ–‡ä»¶
            all_files = []
            for root, _dirs, files in os.walk(resolved_dir):
                for file in files:
                    if file.endswith((".md", ".json")):
                        file_path = os.path.join(root, file)
                        all_files.append(file_path)

            logger.info(f"ResearchIntegrationTool: å‘ç°çš„æ‰€æœ‰æ–‡ä»¶: {all_files}")

            # è¿‡æ»¤æ‰æœ€ç»ˆæŠ¥å‘Šæ–‡ä»¶
            research_files = []
            for file_path in all_files:
                filename = os.path.basename(file_path)
                # æ’é™¤ final_report*.md æ–‡ä»¶
                if not filename.startswith("final_report"):
                    research_files.append(file_path)

            logger.info(f"ResearchIntegrationTool: è¿‡æ»¤åçš„ç ”ç©¶æ–‡ä»¶: {research_files}")
            
            # å¦‚æœå‘ç°çš„æ–‡ä»¶æ•°é‡å¼‚å¸¸å°‘ï¼Œå°è¯•ç­‰å¾…å¹¶é‡æ–°æ‰«æ
            if len(research_files) < 2:
                logger.warning(f"ResearchIntegrationTool: åªå‘ç° {len(research_files)} ä¸ªç ”ç©¶æ–‡ä»¶ï¼Œå¯èƒ½å­˜åœ¨æ–‡ä»¶ç”Ÿæˆå»¶è¿Ÿ")
                import time
                time.sleep(2)  # ç­‰å¾…2ç§’
                
                # é‡æ–°æ‰«æ
                all_files = []
                for root, _dirs, files in os.walk(resolved_dir):
                    for file in files:
                        if file.endswith((".md", ".json")):
                            file_path = os.path.join(root, file)
                            all_files.append(file_path)
                
                research_files = []
                for file_path in all_files:
                    filename = os.path.basename(file_path)
                    if not filename.startswith("final_report"):
                        research_files.append(file_path)
                
                logger.info(f"ResearchIntegrationTool: é‡æ–°æ‰«æåçš„ç ”ç©¶æ–‡ä»¶: {research_files}")

            return research_files

        except OSError as e:
            raise ToolError(f"æ–‡ä»¶å‘ç°å¤±è´¥: {e!s}")

    async def _read_and_analyze_research(
        self, file_paths: list[str], llm_client=None
    ) -> dict[str, Any]:
        """è¯»å–å¹¶åˆ†æç ”ç©¶æ–‡ä»¶å†…å®¹ã€‚"""
        research_data = {
            "files": [],
            "content_summary": {},
            "key_findings": [],
            "structured_data": {},
        }

        for file_path in file_paths:
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_path, encoding="utf-8") as f:
                    content = f.read()

                filename = os.path.basename(file_path)

                # åˆ†ææ–‡ä»¶å†…å®¹
                analysis = await self._analyze_file_content(filename, content, llm_client)

                research_data["files"].append(
                    {
                        "path": file_path,
                        "filename": filename,
                        "content": content,
                        "analysis": analysis,
                    }
                )

                # æå–å…³é”®ä¿¡æ¯
                if analysis.get("key_findings"):
                    research_data["key_findings"].extend(analysis["key_findings"])

                if analysis.get("structured_data"):
                    research_data["structured_data"][filename] = analysis["structured_data"]

            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
                continue

        return research_data

    async def _analyze_file_content(
        self, filename: str, content: str, llm_client=None
    ) -> dict[str, Any]:
        """ä½¿ç”¨LLMåˆ†ææ–‡ä»¶å†…å®¹ã€‚"""
        if not llm_client:
            # ç®€å•çš„å…³é”®è¯æå–ä½œä¸ºåå¤‡
            return self._simple_content_analysis(filename, content)

        try:
            # ä½¿ç”¨ä¸“é—¨çš„JSONæ¨¡å¼è°ƒç”¨LLM
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œå¿…é¡»è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼å“åº”ã€‚",
                },
                {
                    "role": "user",
                    "content": f"""
åˆ†æä»¥ä¸‹ç ”ç©¶æ–‡ä»¶çš„å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

æ–‡ä»¶å: {filename}
å†…å®¹: {content[:3000]}...

è¯·æå–ï¼š
1. å…³é”®å‘ç°å’Œç»“è®º
2. å…·ä½“æ•°æ®ï¼ˆä»·æ ¼ã€æ—¶é—´ã€åœ°ç‚¹ç­‰ï¼‰
3. é‡è¦å»ºè®®å’Œæ³¨æ„äº‹é¡¹
4. æ–‡ä»¶ç±»å‹å’Œä¸»é¢˜

ç‰¹åˆ«æ³¨æ„ï¼š
- å¦‚æœæ˜¯JSONæ–‡ä»¶ï¼Œæ·±åº¦æå–æ‰€æœ‰ç»“æ„åŒ–æ•°æ®ï¼ˆä»·æ ¼ã€æ—¶é—´ã€åœ°ç‚¹ã€æ”¿ç­–ç­‰ï¼‰
- å¦‚æœæ˜¯Markdownæ–‡ä»¶ï¼Œæå–æ‰€æœ‰æ–‡æœ¬ä¿¡æ¯å’Œæ ¼å¼
- ç¡®ä¿ä¸é—æ¼ä»»ä½•é‡è¦çš„ä»·æ ¼ã€è´¹ç”¨ã€é—¨ç¥¨ä¿¡æ¯

è¿”å›JSONæ ¼å¼ï¼š
{{
    "file_type": "æ–‡ä»¶ç±»å‹",
    "theme": "ä¸»é¢˜",
    "key_findings": ["å‘ç°1", "å‘ç°2"],
    "specific_data": {{"ä»·æ ¼": "xxx", "æ—¶é—´": "xxx"}},
    "recommendations": ["å»ºè®®1", "å»ºè®®2"],
    "summary": "å†…å®¹æ‘˜è¦"
}}
""",
                },
            ]

            response = await llm_client.complete_chat_json(messages)

            # éªŒè¯å“åº”ç»“æ„
            if not isinstance(response, dict):
                raise ValueError("LLM response is not a dictionary")

            return response

        except Exception as e:
            print(f"LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ†æ: {e}")
            return self._simple_content_analysis(filename, content)

    def _simple_content_analysis(self, filename: str, content: str) -> dict[str, Any]:
        """ç®€å•çš„å…³é”®è¯åˆ†æä½œä¸ºåå¤‡æ–¹æ¡ˆã€‚"""
        analysis = {
            "file_type": "research",
            "theme": filename.replace(".md", "").replace(".json", ""),
            "key_findings": [],
            "specific_data": {},
            "recommendations": [],
            "summary": content[:200] + "..." if len(content) > 200 else content,
        }

        # æå–ä»·æ ¼ä¿¡æ¯
        price_patterns = [
            r"(\d+)\s*æ¾³å…ƒ",
            r"(\d+)\s*å…ƒ",
            r"ä»·æ ¼[ï¼š:]\s*(\d+)",
            r"è´¹ç”¨[ï¼š:]\s*(\d+)",
        ]

        for pattern in price_patterns:
            matches = re.findall(pattern, content)
            if matches:
                analysis["specific_data"]["ä»·æ ¼"] = matches[0]
                break

        # æå–æ—¶é—´ä¿¡æ¯
        time_patterns = [r"(\d+)\s*å°æ—¶", r"(\d+)\s*å¤©", r"(\d+)\s*å‘¨", r"(\d+)\s*æœˆ"]

        for pattern in time_patterns:
            matches = re.findall(pattern, content)
            if matches:
                analysis["specific_data"]["æ—¶é—´"] = matches[0]
                break

        return analysis

    async def _generate_comprehensive_report(
        self, research_data: dict[str, Any], original_goal: str, llm_client=None
    ) -> str:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Šã€‚"""
        if not llm_client:
            return await self._generate_simple_report(research_data, original_goal, llm_client)

        try:
            research_content = self._extract_research_content(research_data)
            
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶æŠ¥å‘Šæ•´åˆä¸“å®¶ï¼Œæ“…é•¿æ·±åº¦åˆ†æç ”ç©¶èµ„æ–™ï¼Œæ ¹æ®ç”¨æˆ·ç›®æ ‡ç”Ÿæˆè¯¦å°½ã€å‡†ç¡®ã€å®ç”¨çš„ç»¼åˆæŠ¥å‘Šã€‚ä½ å¿…é¡»å……åˆ†åˆ©ç”¨æ‰€æœ‰ç ”ç©¶æ•°æ®ï¼Œä¸é—æ¼ä»»ä½•é‡è¦ä¿¡æ¯ã€‚",
                },
                {
                    "role": "user",
                    "content": f"""
è¯·åŸºäºä»¥ä¸‹å®Œæ•´çš„ç ”ç©¶èµ„æ–™ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½è¯¦å°½çš„ç»¼åˆæŠ¥å‘Šã€‚

**ç”¨æˆ·ç›®æ ‡**ï¼š{original_goal}

===== å®Œæ•´ç ”ç©¶èµ„æ–™ =====
{research_content}
===== ç ”ç©¶èµ„æ–™ç»“æŸ =====

## ä½ çš„ä»»åŠ¡

æ·±åº¦åˆ†æä»¥ä¸Šæ‰€æœ‰ç ”ç©¶èµ„æ–™ï¼Œç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´ã€è¯¦ç»†ã€å‡†ç¡®çš„Markdownæ ¼å¼ç»¼åˆæŠ¥å‘Šã€‚

## æŠ¥å‘Šç”Ÿæˆè¦æ±‚

### 1. æ™ºèƒ½ç†è§£ç”¨æˆ·æ„å›¾
- ä»”ç»†åˆ†æç”¨æˆ·ç›®æ ‡ï¼Œç†è§£ä»»åŠ¡ç±»å‹ï¼ˆæ—…è¡Œè§„åˆ’ã€æŠ€æœ¯ç ”ç©¶ã€å­¦ä¹ è®¡åˆ’ã€å•†ä¸šåˆ†æã€äº§å“å¯¹æ¯”ç­‰ï¼‰
- æ ¹æ®ä»»åŠ¡ç±»å‹ï¼Œè‡ªåŠ¨è°ƒæ•´æŠ¥å‘Šç»“æ„å’Œå†…å®¹é‡ç‚¹
- ç¡®ä¿æŠ¥å‘Šå†…å®¹ä¸ç”¨æˆ·ç›®æ ‡é«˜åº¦ç›¸å…³

### 2. å……åˆ†åˆ©ç”¨ç ”ç©¶æ•°æ®
- **å®Œæ•´æ€§**ï¼šæå–ç ”ç©¶èµ„æ–™ä¸­çš„æ‰€æœ‰å…³é”®ä¿¡æ¯ã€æ•°æ®ã€å‘ç°ã€å»ºè®®
- **å‡†ç¡®æ€§**ï¼šæ‰€æœ‰æ•°æ®ã€æ—¶é—´ã€ä»·æ ¼ã€åç§°ç­‰å¿…é¡»æ¥è‡ªç ”ç©¶èµ„æ–™ï¼Œä¸è¦ç¼–é€ 
- **ç³»ç»Ÿæ€§**ï¼šå°†åˆ†æ•£çš„ä¿¡æ¯è¿›è¡Œé€»è¾‘æ€§æ•´åˆï¼Œå½¢æˆè¿è´¯çš„çŸ¥è¯†ä½“ç³»
- **æ·±åº¦æ€§**ï¼šä¸ä»…ç½—åˆ—ä¿¡æ¯ï¼Œè¿˜è¦æä¾›åˆ†æã€å¯¹æ¯”ã€å»ºè®®
- **ğŸ”¥ JSONæ•°æ®é‡ç‚¹**ï¼šç‰¹åˆ«å…³æ³¨JSONæ–‡ä»¶ä¸­çš„ç»“æ„åŒ–æ•°æ®ï¼Œå¦‚ä»·æ ¼ã€æ—¶é—´ã€åœ°ç‚¹ç­‰ï¼Œå¿…é¡»è¯¦ç»†æå–å¹¶æ•´åˆåˆ°æŠ¥å‘Šä¸­
- **ğŸš¨ å¼ºåˆ¶è¦æ±‚**ï¼šæ¯ä¸ªå­ä»»åŠ¡çš„ç ”ç©¶ç»“æœéƒ½å¿…é¡»åœ¨æŠ¥å‘Šä¸­ä½“ç°ï¼Œä¸èƒ½é—æ¼ä»»ä½•é‡è¦ä¿¡æ¯
- **ğŸ“Š å·¥å…·è°ƒç”¨ç»“æœ**ï¼šæ‰€æœ‰å·¥å…·è°ƒç”¨ï¼ˆå¦‚brave_searchã€write_fileç­‰ï¼‰äº§ç”Ÿçš„æ•°æ®éƒ½å¿…é¡»å……åˆ†åˆ©ç”¨

### 3. ç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Šç»“æ„

æ ¹æ®ç”¨æˆ·ç›®æ ‡å’Œç ”ç©¶å†…å®¹ï¼Œæ™ºèƒ½ç»„ç»‡æŠ¥å‘Šç»“æ„ã€‚å‚è€ƒä½†ä¸é™äºä»¥ä¸‹æ¡†æ¶ï¼š

**é€šç”¨ç»“æ„æ¡†æ¶**ï¼š
- æ¦‚è¿°å’ŒèƒŒæ™¯
- æ ¸å¿ƒå‘ç°/ä¸»è¦å†…å®¹
- è¯¦ç»†åˆ†æï¼ˆåˆ†ä¸»é¢˜ã€åˆ†é˜¶æ®µã€åˆ†ç±»åˆ«ç­‰ï¼‰
- æ•°æ®å¯¹æ¯”å’Œè¯„ä¼°
- å®æ–½å»ºè®®/è¡ŒåŠ¨æ–¹æ¡ˆ
- æ³¨æ„äº‹é¡¹å’Œé£é™©
- æ€»ç»“å’Œå±•æœ›

**æ ¹æ®å…·ä½“ä»»åŠ¡ç±»å‹è°ƒæ•´**ï¼š
- æ—…è¡Œè®¡åˆ’ â†’ è¡Œç¨‹å®‰æ’ã€äº¤é€šä½å®¿ã€é¢„ç®—ã€å®ç”¨ä¿¡æ¯
- æŠ€æœ¯ç ”ç©¶ â†’ æŠ€æœ¯æ–¹æ¡ˆã€å®ç°ç»†èŠ‚ã€æ€§èƒ½å¯¹æ¯”ã€æœ€ä½³å®è·µ
- å­¦ä¹ è®¡åˆ’ â†’ å­¦ä¹ è·¯çº¿ã€æ—¶é—´å®‰æ’ã€èµ„æºæ¨èã€è¯„ä¼°æ–¹å¼
- å•†ä¸šåˆ†æ â†’ å¸‚åœºåˆ†æã€ç«äº‰æ ¼å±€ã€SWOTã€æˆ˜ç•¥å»ºè®®
- äº§å“å¯¹æ¯” â†’ åŠŸèƒ½å¯¹æ¯”ã€ä»·æ ¼åˆ†æã€é€‚ç”¨åœºæ™¯ã€é€‰è´­å»ºè®®

### 4. æ ¼å¼å’Œå‘ˆç°

- **Markdownæ ¼å¼**ï¼šä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å±‚çº§ï¼ˆ# ## ###ï¼‰ã€åˆ—è¡¨ï¼ˆ- *ï¼‰ã€è¡¨æ ¼ç­‰
- **ç»“æ„åŒ–**ï¼šä¿¡æ¯åˆ†ç±»æ˜ç¡®ï¼Œå±‚æ¬¡æ¸…æ™°ï¼Œæ˜“äºé˜…è¯»å’ŒæŸ¥æ‰¾
- **ä¸“ä¸šæ€§**ï¼šè¯­è¨€å‡†ç¡®ã€é€»è¾‘ä¸¥å¯†ã€è¡¨è¾¾æ¸…æ™°
- **å®ç”¨æ€§**ï¼šæä¾›å¯æ“ä½œçš„å»ºè®®ï¼Œè€Œä¸æ˜¯æ³›æ³›è€Œè°ˆ
- **å®Œæ•´æ€§**ï¼šæŠ¥å‘Šåº”è¯¥æ˜¯è‡ªåŒ…å«çš„ï¼Œè¯»è€…æ— éœ€æŸ¥é˜…åŸå§‹èµ„æ–™å³å¯ç†è§£å…¨éƒ¨å†…å®¹

### 5. è´¨é‡æ ‡å‡†

- âœ… æŠ¥å‘Šæ ‡é¢˜å‡†ç¡®åæ˜ ç”¨æˆ·ç›®æ ‡
- âœ… å†…å®¹å®Œæ•´è¦†ç›–æ‰€æœ‰ç ”ç©¶å‘ç°
- âœ… æ•°æ®å‡†ç¡®ï¼Œæ¥æºäºç ”ç©¶èµ„æ–™
- âœ… ç»“æ„åˆç†ï¼Œé€»è¾‘æ¸…æ™°
- âœ… å»ºè®®å…·ä½“ï¼Œå¯æ“ä½œæ€§å¼º
- âœ… æ— å†—ä½™ä¿¡æ¯ï¼Œæ— é—æ¼è¦ç‚¹
- âœ… **ğŸ”¥ å¿…é¡»åŒ…å«æ‰€æœ‰å­ä»»åŠ¡ç»“æœ**ï¼šæ¯ä¸ªå­ä»»åŠ¡çš„ç ”ç©¶æˆæœéƒ½è¦åœ¨æŠ¥å‘Šä¸­ä½“ç°
- âœ… **ğŸ“Š å¿…é¡»åˆ©ç”¨æ‰€æœ‰å·¥å…·è°ƒç”¨æ•°æ®**ï¼šJSONæ–‡ä»¶ã€æœç´¢ç»“æœã€åˆ†ææ•°æ®ç­‰éƒ½è¦å……åˆ†åˆ©ç”¨
- âœ… **ğŸ’° ä»·æ ¼ä¿¡æ¯å¿…é¡»è¯¦ç»†**ï¼šæ‰€æœ‰ä»·æ ¼ã€è´¹ç”¨ã€é—¨ç¥¨ä¿¡æ¯éƒ½è¦å…·ä½“åˆ—å‡º
- âœ… **ğŸ“ åœ°ç‚¹ä¿¡æ¯å¿…é¡»å®Œæ•´**ï¼šæ‰€æœ‰æ™¯ç‚¹ã€åŸå¸‚ã€åœ°å€ä¿¡æ¯éƒ½è¦åŒ…å«

## è¾“å‡ºæ ¼å¼

ç›´æ¥è¾“å‡ºå®Œæ•´çš„Markdownæ ¼å¼æŠ¥å‘Šï¼Œä»¥ `# [æ ‡é¢˜]` å¼€å¤´ã€‚
ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§çš„å‰è¨€ã€åè®°æˆ–å…ƒä¿¡æ¯ã€‚

## ğŸš¨ ç”Ÿæˆå‰å¼ºåˆ¶æ£€æŸ¥æ¸…å•

åœ¨ç”ŸæˆæŠ¥å‘Šå‰ï¼Œè¯·ç¡®è®¤ï¼š
1. âœ… æ˜¯å¦åŒ…å«äº†æ‰€æœ‰å­ä»»åŠ¡çš„ç ”ç©¶ç»“æœï¼Ÿ
2. âœ… æ˜¯å¦å……åˆ†åˆ©ç”¨äº†æ‰€æœ‰JSONæ–‡ä»¶ä¸­çš„ä»·æ ¼ã€æ—¶é—´ã€åœ°ç‚¹æ•°æ®ï¼Ÿ
3. âœ… æ˜¯å¦æå–äº†æ‰€æœ‰å·¥å…·è°ƒç”¨äº§ç”Ÿçš„å…·ä½“ä¿¡æ¯ï¼Ÿ
4. âœ… æ˜¯å¦é—æ¼äº†ä»»ä½•é‡è¦çš„ä»·æ ¼ã€è´¹ç”¨ã€é—¨ç¥¨ä¿¡æ¯ï¼Ÿ
5. âœ… æ˜¯å¦é—æ¼äº†ä»»ä½•æ™¯ç‚¹ã€åŸå¸‚ã€åœ°å€ä¿¡æ¯ï¼Ÿ
6. âœ… æŠ¥å‘Šæ˜¯å¦çœŸæ­£å®ç”¨ï¼Œç”¨æˆ·èƒ½å¦ç›´æ¥ä½¿ç”¨ï¼Ÿ

**è®°ä½ï¼šç”¨æˆ·éœ€è¦çš„æ˜¯å®Œæ•´ã€è¯¦ç»†ã€å®ç”¨çš„æŠ¥å‘Šï¼Œä¸æ˜¯ç®€å•çš„ä¿¡æ¯ç½—åˆ—ï¼**

ç°åœ¨è¯·å¼€å§‹ç”Ÿæˆè¿™ä»½è¯¦å°½çš„ç»¼åˆæŠ¥å‘Šã€‚
""",
                },
            ]

            response = await llm_client.complete_with_context(messages)
            
            if isinstance(response, dict):
                return response.get("content", str(response))
            
            return str(response)

        except Exception as e:
            print(f"LLMæŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æŠ¥å‘Š: {e}")
            return await self._generate_simple_report(research_data, original_goal, llm_client)
    
    def _extract_research_content(self, research_data: dict[str, Any]) -> str:
        """æå–ç ”ç©¶æ–‡ä»¶çš„åŸå§‹å†…å®¹ã€‚"""
        content_parts = []
        
        for file_info in research_data.get("files", []):
            filename = file_info.get("filename", "æœªçŸ¥æ–‡ä»¶")
            content = file_info.get("content", "")
            
            if content.strip():
                content_parts.append(f"## æ–‡ä»¶ï¼š{filename}\n\n{content}\n\n---\n")
        
        return "\n".join(content_parts) if content_parts else "æ— ç ”ç©¶æ•°æ®"

    async def _generate_simple_report(
        self, research_data: dict[str, Any], original_goal: str, llm_client=None
    ) -> str:
        """ç”Ÿæˆç®€å•çš„ç»¼åˆæŠ¥å‘Šã€‚"""
        # ä½¿ç”¨LLMæ™ºèƒ½æå–æ ‡é¢˜
        title = await self._extract_title_with_llm(original_goal, llm_client)

        report_lines = [
            f"# {title}",
            "",
            f"**ç›®æ ‡**: {original_goal}",
            "",
            "## è®¡åˆ’æ¦‚è¿°",
            "",
        ]

        # æå–å…³é”®ä¿¡æ¯
        key_findings = []
        specific_data = {}

        for file_info in research_data["files"]:
            analysis = file_info.get("analysis", {})
            if analysis.get("key_findings"):
                key_findings.extend(analysis["key_findings"])
            if analysis.get("specific_data"):
                specific_data.update(analysis["specific_data"])

        # æ·»åŠ å…³é”®å‘ç°
        if key_findings:
            report_lines.extend(["## ä¸»è¦å‘ç°", ""])
            for finding in key_findings[:10]:  # é™åˆ¶æ•°é‡
                report_lines.append(f"- {finding}")
            report_lines.append("")

        # æ·»åŠ å…·ä½“æ•°æ®
        if specific_data:
            report_lines.extend(["## é‡è¦ä¿¡æ¯", ""])
            for key, value in specific_data.items():
                report_lines.append(f"- **{key}**: {value}")
            report_lines.append("")

        # æ·»åŠ æ–‡ä»¶å†…å®¹æ‘˜è¦
        report_lines.extend(["## è¯¦ç»†è®¡åˆ’", ""])
        for file_info in research_data["files"]:
            filename = file_info["filename"]
            content = file_info["content"]

            # æå–æ–‡ä»¶æ ‡é¢˜
            lines = content.split("\n")
            file_title = lines[0] if lines and lines[0].startswith("#") else filename
            if file_title.startswith("#"):
                file_title = file_title[1:].strip()

            report_lines.append(f"### {file_title}")
            report_lines.append("")

            # æ·»åŠ å†…å®¹æ‘˜è¦ï¼ˆå‰å‡ æ®µï¼‰
            paragraphs = content.split("\n\n")
            for para in paragraphs[:3]:  # åªå–å‰3æ®µ
                if para.strip() and not para.startswith("#"):
                    report_lines.append(para.strip())
                    report_lines.append("")

            report_lines.append("---")
            report_lines.append("")

        return "\n".join(report_lines)

    def _extract_title_from_goal(self, goal: str) -> str:
        """ä»ç›®æ ‡ä¸­æå–åˆé€‚çš„æ ‡é¢˜ã€‚"""
        # å¦‚æœç›®æ ‡å¾ˆçŸ­ï¼Œç›´æ¥ä½¿ç”¨
        if len(goal) <= 20:
            return goal

        # ç®€å•çš„å…³é”®è¯æå–ä½œä¸ºåå¤‡
        goal_clean = (
            goal.replace("ç”Ÿæˆ", "")
            .replace("åˆ¶å®š", "")
            .replace("åˆ›å»º", "")
            .replace("çš„", "")
            .strip()
        )

        # å¦‚æœç›®æ ‡åŒ…å«"æŠ¥å‘Š"ï¼Œæå–æŠ¥å‘Šå‰çš„éƒ¨åˆ†
        if "æŠ¥å‘Š" in goal_clean:
            goal_clean = goal_clean.split("æŠ¥å‘Š")[0].strip()

        # å¦‚æœç›®æ ‡åŒ…å«"è®¡åˆ’"ï¼Œæå–è®¡åˆ’å‰çš„éƒ¨åˆ†
        if "è®¡åˆ’" in goal_clean:
            goal_clean = goal_clean.split("è®¡åˆ’")[0].strip()

        # å¦‚æœç›®æ ‡å¤ªé•¿ï¼Œæˆªå–å‰30ä¸ªå­—ç¬¦
        if len(goal_clean) > 30:
            goal_clean = goal_clean[:30] + "..."

        # å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ ‡é¢˜
        if not goal_clean:
            goal_clean = "ç»¼åˆè®¡åˆ’æŠ¥å‘Š"

        return goal_clean

    async def _extract_title_with_llm(self, goal: str, llm_client=None) -> str:
        """ä½¿ç”¨LLMæ™ºèƒ½æå–æ ‡é¢˜ã€‚"""
        if not llm_client:
            return self._extract_title_from_goal(goal)

        try:
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ ‡é¢˜ç”ŸæˆåŠ©æ‰‹ï¼Œèƒ½å¤Ÿä»ç”¨æˆ·ç›®æ ‡ä¸­æå–ç®€æ´ã€å‡†ç¡®çš„æ ‡é¢˜ã€‚",
                },
                {
                    "role": "user",
                    "content": f"""
è¯·ä»ä»¥ä¸‹ç”¨æˆ·ç›®æ ‡ä¸­æå–ä¸€ä¸ªç®€æ´ã€å‡†ç¡®çš„æ ‡é¢˜ï¼ˆä¸è¶…è¿‡20ä¸ªå­—ç¬¦ï¼‰ï¼š

ç”¨æˆ·ç›®æ ‡: {goal}

è¦æ±‚ï¼š
1. æ ‡é¢˜åº”è¯¥ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡20ä¸ªå­—ç¬¦
2. å»é™¤"ç”Ÿæˆ"ã€"åˆ¶å®š"ã€"åˆ›å»º"ç­‰åŠ¨è¯
3. ä¿ç•™æ ¸å¿ƒå†…å®¹
4. å¦‚æœç›®æ ‡æ˜¯å…³äºæŠ¥å‘Šæˆ–è®¡åˆ’ï¼Œæå–ä¸»è¦å†…å®¹éƒ¨åˆ†
5. åªè¿”å›æ ‡é¢˜ï¼Œä¸è¦å…¶ä»–è§£é‡Š

ç¤ºä¾‹ï¼š
- "ç”Ÿæˆä¸‰ä¸ªæœˆå­¦ä¹ è®¡åˆ’æŠ¥å‘Š" â†’ "ä¸‰ä¸ªæœˆå­¦ä¹ è®¡åˆ’"
- "åˆ¶å®šå…¬å¸å¹´åº¦è¥é”€ç­–ç•¥" â†’ "å…¬å¸å¹´åº¦è¥é”€ç­–ç•¥"
- "åˆ›å»ºç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ" â†’ "ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ"
""",
                },
            ]

            response = await llm_client.complete_chat_json(messages)

            if isinstance(response, dict) and "content" in response:
                title = response["content"].strip()
                # ç¡®ä¿æ ‡é¢˜ä¸ä¼šå¤ªé•¿
                if len(title) > 30:
                    title = title[:30] + "..."
                return title
            else:
                return self._extract_title_from_goal(goal)

        except Exception as e:
            print(f"LLMæ ‡é¢˜æå–å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•: {e}")
            return self._extract_title_from_goal(goal)

    async def _save_report(self, content: str, output_file: str) -> None:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ã€‚"""
        try:
            resolved_output = self._resolve_task_directory(output_file)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(resolved_output)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)

            # å†™å…¥æ–‡ä»¶
            with open(resolved_output, "w", encoding="utf-8") as f:
                f.write(content)

        except Exception as e:
            raise ToolError(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e!s}")

    def execute_sync(
        self, task_directory: str, output_file: str, original_goal: str = "", llm_client=None
    ) -> str:
        """åŒæ­¥ç‰ˆæœ¬çš„ç ”ç©¶æ•´åˆã€‚"""
        import asyncio

        return asyncio.run(self.execute(task_directory, output_file, original_goal, llm_client))
