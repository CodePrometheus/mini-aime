from __future__ import annotations

from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import asyncio
import json
import logging
import os
from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path

from fastapi import BackgroundTasks, Body, FastAPI
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles
from starlette.responses import FileResponse

from src.config.logging import setup_logging
from src.config.settings import settings
from src.core.mini_aime import MiniAime, MiniAimeConfig
from src.llm.base import OpenAICompatibleClient


# å…¨å±€ä»»åŠ¡é›†åˆï¼Œç”¨äºå­˜å‚¨åå°ä»»åŠ¡å¼•ç”¨
background_tasks_set: set[asyncio.Task] = set()


try:
    from prometheus_client import CONTENT_TYPE_LATEST, generate_latest
except Exception:  # pragma: no cover
    generate_latest = None  # type: ignore
    CONTENT_TYPE_LATEST = "text/plain; version=0.0.4; charset=utf-8"  # type: ignore


setup_logging()
logger = logging.getLogger("mini_aime.api")

# å­˜å‚¨æ´»è·ƒçš„ä»»åŠ¡
active_tasks = {}

# ä»»åŠ¡çŠ¶æ€æŒä¹…åŒ–æ–‡ä»¶è·¯å¾„
ACTIVE_TASKS_FILE = "logs/active_tasks.json"


def load_active_tasks():
    """ä»æ–‡ä»¶åŠ è½½æ´»è·ƒä»»åŠ¡çŠ¶æ€"""
    global active_tasks
    try:
        if os.path.exists(ACTIVE_TASKS_FILE):
            with open(ACTIVE_TASKS_FILE, encoding="utf-8") as f:
                loaded_tasks = json.load(f)
                active_tasks.update(loaded_tasks)  # ä½¿ç”¨ update è€Œä¸æ˜¯ç›´æ¥èµ‹å€¼
            logger.info(f"Loaded {len(loaded_tasks)} active tasks from file")
    except Exception as e:
        logger.warning(f"Failed to load active tasks: {e}")
        # ä¸è¦æ¸…ç©º active_tasksï¼Œä¿æŒç°æœ‰çŠ¶æ€


def save_active_tasks():
    """ä¿å­˜æ´»è·ƒä»»åŠ¡çŠ¶æ€åˆ°æ–‡ä»¶"""
    try:
        os.makedirs(os.path.dirname(ACTIVE_TASKS_FILE), exist_ok=True)

        # åˆ›å»ºå¯åºåˆ—åŒ–çš„ä»»åŠ¡å‰¯æœ¬ï¼ˆæ’é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼‰
        serializable_tasks = {}
        for task_id, task_info in active_tasks.items():
            serializable_task = {k: v for k, v in task_info.items() if k != "progress_manager"}
            serializable_tasks[task_id] = serializable_task

        with open(ACTIVE_TASKS_FILE, "w", encoding="utf-8") as f:
            json.dump(serializable_tasks, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Failed to save active tasks: {e}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ã€‚"""
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    logger.info("API starting up", extra={"debug": settings.debug})
    load_active_tasks()  # åŠ è½½æŒä¹…åŒ–çš„ä»»åŠ¡çŠ¶æ€
    yield
    # å…³é—­æ—¶æ‰§è¡Œï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
    save_active_tasks()  # ä¿å­˜ä»»åŠ¡çŠ¶æ€
    logger.info("API shutting down")


app = FastAPI(title="Mini-Aime API", debug=settings.debug, lifespan=lifespan)


@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ã€‚"""
    return {"status": "healthy", "service": "mini-aime-api"}


@app.get("/api/tasks/active")
async def get_active_tasks():
    """è·å–å½“å‰æ´»è·ƒçš„ä»»åŠ¡åˆ—è¡¨ã€‚"""
    active_task_list = []
    for task_id, task_info in active_tasks.items():
        active_task_list.append(
            {
                "task_id": task_id,
                "status": task_info["status"],
                "goal": task_info.get("goal", ""),
                "created_at": task_info.get("created_at", ""),
            }
        )

    return {"active_tasks": active_task_list, "count": len(active_task_list)}


async def execute_task_background(task_id: str, goal: str, log_file: Path):
    """åå°æ‰§è¡Œä»»åŠ¡ã€‚"""
    llm_client = None
    file_handler = None
    
    try:
        # é…ç½®æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶
        file_handler = logging.FileHandler(log_file, encoding="utf-8")
        file_handler.setLevel(logging.INFO)
        file_formatter = logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s")
        file_handler.setFormatter(file_formatter)

        # æ·»åŠ æ–‡ä»¶handleråˆ°æ‰€æœ‰ç›¸å…³logger
        loggers_to_configure = [
            logging.getLogger("mini_aime"),
            logging.getLogger("src.core"),
            logging.getLogger("src.llm"),
        ]
        for log in loggers_to_configure:
            log.addHandler(file_handler)

        # åˆ›å»ºå¹¶æ‰§è¡Œä»»åŠ¡
        llm_client = OpenAICompatibleClient()
        config = MiniAimeConfig()
        mini_aime = MiniAime(llm_client=llm_client, config=config)

        # ä¿å­˜ progress_manager å¼•ç”¨ä¾›äº‹ä»¶æµä½¿ç”¨
        active_tasks[task_id]["progress_manager"] = mini_aime.progress_manager
        active_tasks[task_id]["status"] = "running"
        save_active_tasks()  # ä¿å­˜çŠ¶æ€æ›´æ–°

        logger.info(f"Starting task {task_id}: {goal}")

        # å°† task_id ä½œä¸º session_id ä¼ å…¥ï¼Œç¡®ä¿ç›®å½•ä¸€è‡´
        await mini_aime.execute_task(user_goal=goal, session_id=task_id)

        active_tasks[task_id]["status"] = "completed"
        save_active_tasks()  # ä¿å­˜çŠ¶æ€æ›´æ–°
        logger.info(f"Task {task_id} completed successfully")

    except Exception as e:
        logger.error(f"Task {task_id} failed: {e}", exc_info=True)
        active_tasks[task_id]["status"] = "failed"
        active_tasks[task_id]["error"] = str(e)
        save_active_tasks()  # ä¿å­˜çŠ¶æ€æ›´æ–°
    finally:
        # å…³é—­ LLM å®¢æˆ·ç«¯
        if llm_client:
            try:
                await llm_client.close()
            except Exception as e:
                logger.warning(f"Failed to close LLM client: {e}")
        
        # ç§»é™¤æ–‡ä»¶handler
        if file_handler:
            for log in loggers_to_configure:
                log.removeHandler(file_handler)
            file_handler.close()


@app.get("/health")
async def health() -> dict:
    logger.debug("Health check called")
    return {"status": "ok"}


@app.get("/metrics")
async def metrics():  # type: ignore[override]
    if not settings.enable_metrics:
        return {"detail": "metrics disabled"}
    if generate_latest is None:
        return {"detail": "prometheus-client not installed"}

    data = generate_latest()
    from fastapi import Response

    return Response(content=data, media_type=CONTENT_TYPE_LATEST)


@app.post("/agent/resolution")
async def submit_resolution(
    task_id: str = Body(..., embed=True),
    resolution_type: str = Body(..., embed=True, description="resume|retry|replan"),
    user_hint: str | None = Body(None, embed=True),
) -> dict:
    """æäº¤ç”¨æˆ·æŒ‡å¼•ç”¨äºæ¢å¤/é‡è¯•/é‡è§„åˆ’ã€‚"""
    if not settings.human_in_loop_enabled:
        return {"detail": "human-in-loop disabled"}

    return {
        "task_id": task_id,
        "resolution_type": resolution_type,
        "user_hint": (user_hint or "").strip(),
        "status": "accepted",
    }


@app.post("/api/tasks/submit")
async def submit_task(
    goal: str = Body(..., embed=True), background_tasks: BackgroundTasks = None
) -> dict:
    """æäº¤æ–°ä»»åŠ¡å¹¶è¿”å›ä»»åŠ¡IDã€‚"""
    task_id = f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / f"{task_id}.log"

    active_tasks[task_id] = {
        "goal": goal,
        "status": "pending",
        "log_file": str(log_file),
        "started_at": datetime.now().isoformat(),
    }
    save_active_tasks()  # ä¿å­˜æ–°ä»»åŠ¡

    # åœ¨åå°æ‰§è¡Œä»»åŠ¡
    if background_tasks:
        background_tasks.add_task(execute_task_background, task_id, goal, log_file)
    else:
        task = asyncio.create_task(execute_task_background(task_id, goal, log_file))
        background_tasks_set.add(task)
        task.add_done_callback(background_tasks_set.discard)

    logger.info(f"Task submitted: {task_id}, goal: {goal}")

    return {"task_id": task_id, "status": "pending", "message": "Task submitted successfully"}


@app.get("/api/tasks/{task_id}/logs")
async def stream_logs(task_id: str):
    """å®æ—¶æµå¼ä¼ è¾“ä»»åŠ¡æ‰§è¡Œæ—¥å¿—ï¼ˆSSEï¼‰ã€‚"""

    async def log_generator() -> AsyncGenerator[str, None]:
        if task_id not in active_tasks:
            yield "data: {'error': 'Task not found'}\n\n"
            return

        log_file = Path(active_tasks[task_id]["log_file"])

        # ç­‰å¾…æ—¥å¿—æ–‡ä»¶åˆ›å»º
        retry_count = 0
        while not log_file.exists() and retry_count < 100:
            await asyncio.sleep(0.1)
            retry_count += 1

        if not log_file.exists():
            yield "data: {'message': 'Waiting for task to start...'}\n\n"

        # è¯»å–å¹¶æµå¼ä¼ è¾“æ—¥å¿—
        last_position = 0
        no_new_data_count = 0
        first_read = True

        while no_new_data_count < 60:  # 60ç§’æ²¡æœ‰æ–°æ•°æ®å°±æ–­å¼€
            try:
                if log_file.exists():
                    with open(log_file, encoding="utf-8") as f:
                        if first_read:
                            # é¦–æ¬¡è¿æ¥æ—¶ï¼Œè¯»å–æ‰€æœ‰ç°æœ‰æ—¥å¿—
                            all_lines = f.readlines()
                            for line in all_lines:
                                line_clean = line.rstrip("\n\r").replace('"', '\\"')
                                yield f'data: {{"log": "{line_clean}"}}\n\n'
                            last_position = f.tell()
                            first_read = False
                        else:
                            # åç»­åªè¯»å–æ–°å¢çš„æ—¥å¿—
                            f.seek(last_position)
                            new_lines = f.readlines()
                            last_position = f.tell()

                            if new_lines:
                                no_new_data_count = 0
                                for line in new_lines:
                                    # ç§»é™¤æ¢è¡Œç¬¦å¹¶è½¬ä¹‰å¼•å·
                                    line_clean = line.rstrip("\n\r").replace('"', '\\"')
                                    yield f'data: {{"log": "{line_clean}"}}\n\n'
                            else:
                                no_new_data_count += 1
                else:
                    no_new_data_count += 1

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ
                if task_id in active_tasks and active_tasks[task_id]["status"] in (
                    "completed",
                    "failed",
                ):
                    # å†è¯»å–ä¸€æ¬¡ç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½å‘é€äº†
                    await asyncio.sleep(1)
                    if log_file.exists():
                        with open(log_file, encoding="utf-8") as f:
                            f.seek(last_position)
                            final_lines = f.readlines()
                            for line in final_lines:
                                line_clean = line.rstrip("\n\r").replace('"', '\\"')
                                yield f'data: {{"log": "{line_clean}"}}\n\n'
                    break

                await asyncio.sleep(0.5)

            except Exception as e:
                logger.error(f"Error reading log file: {e}")
                yield 'data: {"error": "Error reading logs"}\n\n'
                break

        yield 'data: {"message": "Stream ended"}\n\n'

    return StreamingResponse(
        log_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
        },
    )


@app.get("/api/tasks/{task_id}/events")
async def stream_user_events(task_id: str):
    """å®æ—¶æµå¼ä¼ è¾“ç”¨æˆ·å‹å¥½çš„äº‹ä»¶ï¼ˆSSEï¼‰- æ›¿ä»£åŸå§‹æ—¥å¿—æµã€‚"""

    async def event_generator() -> AsyncGenerator[str, None]:
        if task_id not in active_tasks:
            yield 'data: {"error": "Task not found"}\n\n'
            return

        # è·å– progress_manager å¼•ç”¨
        progress_manager = active_tasks[task_id].get("progress_manager")

        if not progress_manager:
            # å¦‚æœ progress_manager ä¸å­˜åœ¨ï¼ˆæ¯”å¦‚ä»æ–‡ä»¶æ¢å¤çš„ä»»åŠ¡ï¼‰ï¼Œå°è¯•é‡æ–°åˆ›å»º
            logger.info(f"Progress manager not found for task {task_id}, attempting to recreate...")
            try:
                # é‡æ–°åˆ›å»º MiniAime å®ä¾‹æ¥è·å– progress_manager
                llm_client = OpenAICompatibleClient()
                config = MiniAimeConfig()
                mini_aime = MiniAime(llm_client=llm_client, config=config)

                # è®¾ç½®ç›¸åŒçš„ session_id
                mini_aime.progress_manager.set_session_id(task_id)

                # æ›´æ–° active_tasks
                active_tasks[task_id]["progress_manager"] = mini_aime.progress_manager
                progress_manager = mini_aime.progress_manager

                logger.info(f"Successfully recreated progress manager for task {task_id}")
            except Exception as e:
                logger.error(f"Failed to recreate progress manager for task {task_id}: {e}")
                yield 'data: {"error": "Failed to recreate progress manager"}\n\n'
                return

        # å‘é€è¿æ¥ç¡®è®¤
        yield f'data: {{"type": "connected", "task_id": "{task_id}"}}\n\n'

        # é¦–æ¬¡è¿æ¥ï¼šå‘é€æ‰€æœ‰å†å²äº‹ä»¶ï¼ˆæ”¯æŒé¡µé¢åˆ·æ–°åæ¢å¤çŠ¶æ€ï¼‰
        history_events = progress_manager.get_user_event_history()
        if history_events:
            logger.info(
                f"Sending {len(history_events)} historical events to client for task {task_id}"
            )
            for event in history_events:
                event_json = json.dumps(event, ensure_ascii=False)
                yield f"data: {event_json}\n\n"

        # æŒç»­ä» user_event_queue è¯»å–äº‹ä»¶
        no_event_count = 0
        max_wait = 120  # 2åˆ†é’Ÿæ— äº‹ä»¶åˆ™æ–­å¼€

        while no_event_count < max_wait:
            # ä» ProgressManager è·å–ç”¨æˆ·äº‹ä»¶
            user_event = await progress_manager.get_user_event(timeout=1.0)

            if user_event:
                no_event_count = 0
                # å‘é€äº‹ä»¶åˆ°å‰ç«¯
                event_json = json.dumps(user_event, ensure_ascii=False)
                yield f"data: {event_json}\n\n"
            else:
                no_event_count += 1

            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ
            if task_id in active_tasks:
                status = active_tasks[task_id].get("status")
                if status in ("completed", "failed"):
                    # å†ç­‰å¾…ä¸€ä¼šå„¿ç¡®ä¿æ‰€æœ‰äº‹ä»¶éƒ½å‘é€äº†
                    await asyncio.sleep(2)

                    # å°è¯•è¯»å–å‰©ä½™äº‹ä»¶
                    for _ in range(5):
                        event = await progress_manager.get_user_event(timeout=0.5)
                        if event:
                            event_json = json.dumps(event, ensure_ascii=False)
                            yield f"data: {event_json}\n\n"
                        else:
                            break

                    # å‘é€å®Œæˆä¿¡å·
                    yield f'data: {{"type": "stream_ended", "task_status": "{status}"}}\n\n'
                    break

        # è¶…æ—¶
        if no_event_count >= max_wait:
            yield 'data: {"type": "timeout"}\n\n'

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
        },
    )


@app.get("/api/tasks/{task_id}/result")
async def get_task_result(task_id: str) -> dict:
    """è·å–ä»»åŠ¡çš„æœ€ç»ˆç»“æœæ–‡æ¡£ã€‚

    æ ¹æ®è®ºæ–‡ï¼Œæœ€ç»ˆæŠ¥å‘Šåº”è¯¥æ˜¯ï¼š
    1. å„ä¸ªå­ä»»åŠ¡å®Œæˆæ—¶ç”Ÿæˆå­ä»»åŠ¡æŠ¥å‘Š
    2. æ‰€æœ‰ä»»åŠ¡å®Œæˆåï¼Œæ±‡æ€»ç”Ÿæˆæœ€ç»ˆæ•´åˆæŠ¥å‘Š

    è¿”å›ä¼˜å…ˆçº§ï¼š
    1. final_report.md (æ±‡æ€»æŠ¥å‘Š) - æœ€ç»ˆç”¨æˆ·åº”çœ‹åˆ°çš„ç»“æœ
    2. final_report_task_*.md (å­ä»»åŠ¡æŠ¥å‘Š) - ä»…åœ¨æ±‡æ€»æŠ¥å‘Šä¸å­˜åœ¨æ—¶è¿”å›
    """

    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
    if task_id not in active_tasks:
        return {"error": f"Task {task_id} not found"}

    task_info = active_tasks[task_id]
    task_status = task_info.get("status")

    # å¦‚æœä»»åŠ¡è¿˜åœ¨è¿è¡Œï¼Œè¿”å›æç¤º
    if task_status in ("pending", "running"):
        return {
            "task_id": task_id,
            "status": task_status,
            "message": "Task is still running, result not available yet",
        }

    # ä»»åŠ¡å®Œæˆï¼ŒæŸ¥æ‰¾ç»“æœæ–‡ä»¶
    # ç­–ç•¥1: ä¼˜å…ˆæŸ¥æ‰¾ä»»åŠ¡ä¸“å±ç›®å½•ä¸­çš„æ±‡æ€»æŠ¥å‘Š docs/{task_id}/final_report.md
    task_docs_dir = Path("docs") / task_id

    if task_docs_dir.exists():
        # ä¼˜å…ˆæŸ¥æ‰¾ç²¾ç¡®çš„æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶å
        summary_report = task_docs_dir / "final_report.md"
        if summary_report.exists():
            try:
                content = summary_report.read_text(encoding="utf-8")
                return {
                    "task_id": task_id,
                    "filename": summary_report.name,
                    "content": content,
                    "generated_at": datetime.fromtimestamp(
                        summary_report.stat().st_mtime
                    ).isoformat(),
                    "type": "summary_report",  # æ˜ç¡®æ ‡è¯†ä¸ºæ±‡æ€»æŠ¥å‘Š
                    "is_final": True,
                    "source": "task_directory",
                }
            except Exception as e:
                logger.error(f"Error reading summary report {summary_report}: {e}")

        # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶å
        alternative_summary_names = ["final_learning_plan.md", "learning_plan.md", "ç»¼åˆæŠ¥å‘Š.md"]
        for alt_name in alternative_summary_names:
            alt_report = task_docs_dir / alt_name
            if alt_report.exists():
                try:
                    content = alt_report.read_text(encoding="utf-8")
                    return {
                        "task_id": task_id,
                        "filename": alt_report.name,
                        "content": content,
                        "generated_at": datetime.fromtimestamp(
                            alt_report.stat().st_mtime
                        ).isoformat(),
                        "type": "summary_report",
                        "is_final": True,
                        "source": "task_directory",
                    }
                except Exception as e:
                    logger.error(f"Error reading alternative summary {alt_report}: {e}")

        # æŸ¥æ‰¾ä»»æ„ä»¥ final_report å¼€å¤´çš„æŠ¥å‘Šæ–‡ä»¶ï¼ˆä¸é™æ–‡ä»¶åï¼‰
        # ä½†è¦æ’é™¤æ˜æ˜¾çš„å­ä»»åŠ¡æŠ¥å‘Šï¼ˆåŒ…å« _T[æ•°å­—]_ æ¨¡å¼ï¼‰
        all_reports = list(task_docs_dir.glob("final_report_*.md"))
        if all_reports:
            import re

            # è¿‡æ»¤æ‰å­ä»»åŠ¡æŠ¥å‘Šï¼ˆæ–‡ä»¶ååŒ…å« _Tæ•°å­—_ æ¨¡å¼ï¼Œå¦‚ final_report_T1_, final_report_T7_ï¼‰
            summary_candidates = [f for f in all_reports if not re.search(r"_T\d+_", f.name)]

            if summary_candidates:
                latest_file = max(summary_candidates, key=lambda p: p.stat().st_mtime)
                try:
                    content = latest_file.read_text(encoding="utf-8")
                    return {
                        "task_id": task_id,
                        "filename": latest_file.name,
                        "content": content,
                        "generated_at": datetime.fromtimestamp(
                            latest_file.stat().st_mtime
                        ).isoformat(),
                        "type": "summary_report",
                        "is_final": True,
                        "source": "task_directory",
                    }
                except Exception as e:
                    logger.error(f"Error reading summary report {latest_file}: {e}")
            else:
                # å¦‚æœéƒ½æ˜¯å­ä»»åŠ¡æŠ¥å‘Šï¼Œé€‰æ‹©æœ€æ–°çš„ï¼ˆä½†æ ‡è®°ä¸ºéæœ€ç»ˆï¼‰
                latest_file = max(all_reports, key=lambda p: p.stat().st_mtime)
                try:
                    content = latest_file.read_text(encoding="utf-8")
                    return {
                        "task_id": task_id,
                        "filename": latest_file.name,
                        "content": content,
                        "generated_at": datetime.fromtimestamp(
                            latest_file.stat().st_mtime
                        ).isoformat(),
                        "type": "subtask_report",
                        "is_final": False,
                        "warning": "æ˜¾ç¤ºçš„æ˜¯æœ€æ–°çš„å­ä»»åŠ¡æŠ¥å‘Šï¼Œæ±‡æ€»æŠ¥å‘Šå¯èƒ½å°šæœªç”Ÿæˆ",
                        "source": "task_directory",
                    }
                except Exception as e:
                    logger.error(f"Error reading report {latest_file}: {e}")

    # ç­–ç•¥2: å…œåº• - åœ¨æ—§çš„ docs/ ç›®å½•ä¸­æŒ‰æ—¶é—´è¿‡æ»¤æŸ¥æ‰¾
    docs_dir = Path("docs")
    task_start_time_str = task_info.get("started_at")
    if task_start_time_str and docs_dir.exists():
        task_start_time = datetime.fromisoformat(task_start_time_str).timestamp()
        result_files = []
        for pattern in ["final_report_*.md", "docs/final_report_*.md"]:
            for file in docs_dir.glob(pattern):
                file_mtime = file.stat().st_mtime
                if task_start_time <= file_mtime:
                    result_files.append(file)

        if result_files:
            latest_file = max(result_files, key=lambda p: p.stat().st_mtime)
            try:
                content = latest_file.read_text(encoding="utf-8")
                return {
                    "task_id": task_id,
                    "filename": latest_file.name,
                    "content": content,
                    "generated_at": datetime.fromtimestamp(latest_file.stat().st_mtime).isoformat(),
                    "type": "task_result",
                    "source": "legacy_time_filter",
                }
            except Exception as e:
                logger.error(f"Error reading result file {latest_file}: {e}")

    # ç­–ç•¥3: æœ€åå…œåº• - æŸ¥æ‰¾ä»»ä½•æœ€æ–°çš„æŠ¥å‘Šæ–‡ä»¶
    if docs_dir.exists():
        all_reports = list(docs_dir.glob("**/*.md"))
        if all_reports:
            latest_file = max(all_reports, key=lambda p: p.stat().st_mtime)
            try:
                content = latest_file.read_text(encoding="utf-8")
                return {
                    "task_id": task_id,
                    "filename": latest_file.name,
                    "content": content,
                    "generated_at": datetime.fromtimestamp(latest_file.stat().st_mtime).isoformat(),
                    "type": "fallback",
                    "warning": "Using latest available report as fallback",
                }
            except Exception as e:
                logger.error(f"Error reading fallback file: {e}")

    return {"error": f"No result found for task {task_id}"}


@app.get("/api/tasks/{task_id}/status")
async def get_task_status(task_id: str) -> dict:
    """è·å–ä»»åŠ¡çŠ¶æ€ã€‚"""
    if task_id not in active_tasks:
        return {"error": "Task not found"}

    return active_tasks[task_id]


# æŒ‚è½½å‰ç«¯é™æ€æ–‡ä»¶
frontend_dir = Path(__file__).parent.parent.parent / "frontend" / "dist"
if frontend_dir.exists():
    app.mount("/assets", StaticFiles(directory=frontend_dir / "assets"), name="assets")

    @app.get("/")
    async def serve_frontend():
        return FileResponse(frontend_dir / "index.html")

    # å¤„ç† favicon
    @app.get("/vite.svg")
    async def serve_favicon():
        favicon_path = frontend_dir / "vite.svg"
        if favicon_path.exists():
            return FileResponse(favicon_path)
        return {"error": "Favicon not found"}
else:

    @app.get("/")
    async def serve_placeholder():
        return {"message": "Frontend not built. Run 'cd frontend && npm run build'"}


def main():
    """å¯åŠ¨ Mini-Aime API æœåŠ¡å™¨çš„ä¸»å‡½æ•°"""
    import uvicorn
    
    try:
        uvicorn.run(
            "src.api.main:app",
            host="0.0.0.0",
            port=8000,
            reload=True,
            log_level="info"
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
